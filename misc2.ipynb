{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b708a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION SANS PRUNING\n",
    "#def objective_focused(trial):\n",
    "#    quant_idx  = trial.suggest_int(\"quant_idx\",  0, len(quantisation_candidates) - 1)\n",
    "#    mono_idx   = trial.suggest_int(\"mono_idx\",   0, len(monotone_constraint_candidates) - 1)\n",
    "#    param = {\n",
    "#        'objective': 'RMSE', 'loss_function': 'RMSE', 'eval_metric': 'RMSE',\n",
    "#        'random_seed': RANDOM_STATE, 'cat_features':  [c for c in selected_rfecv if c in cat_feats_fs],\n",
    "#        'verbose': 0, 'early_stopping_rounds': 50,\n",
    "#\n",
    "#        'iterations': trial.suggest_int('iterations', int(max(100, center_iterations * (1 - GRID_SEARCH_RANGE_FACTOR*1.5))), int(center_iterations * (1 + GRID_SEARCH_RANGE_FACTOR*1.5))), # Slightly wider range than GS\n",
    "#        'learning_rate': trial.suggest_float('learning_rate', max(0.001, center_lr * (1 - GRID_SEARCH_RANGE_FACTOR*1.5)), center_lr * (1 + GRID_SEARCH_RANGE_FACTOR*1.5), log=True),\n",
    "#        'depth': trial.suggest_int('depth', max(2, center_depth - 1), center_depth + 1), # Tight range around best depth\n",
    "#        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', max(0.01, center_l2 * (1 - GRID_SEARCH_RANGE_FACTOR*1.5)), center_l2 * (1 + GRID_SEARCH_RANGE_FACTOR*1.5), log=True),\n",
    "#        'border_count': trial.suggest_int('border_count', int(max(32, center_border * (1 - GRID_SEARCH_RANGE_FACTOR*1.5))), int(min(255, center_border * (1 + GRID_SEARCH_RANGE_FACTOR*1.5)))),\n",
    "#\n",
    "#        'per_float_feature_quantization': quantisation_candidates[quant_idx],\n",
    "#        'monotone_constraints':            monotone_constraint_candidates[mono_idx],}\n",
    "#\n",
    "#    cv = KFold(n_splits=CV_FOLDS_TUNING, shuffle=True, random_state=RANDOM_STATE)\n",
    "#    rmses = []\n",
    "#    for fold, (train_idx, val_idx) in enumerate(cv.split(X_rfecv, y_train)):\n",
    "#        X_t, X_v = X_rfecv.iloc[train_idx], X_rfecv.iloc[val_idx]\n",
    "#        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "#        # Optional Oversampling (inside fold)\n",
    "#        if ENABLE_OVERSAMPLING and HOLIDAY_COLUMN in X_t.columns: pass # Placeholder\n",
    "#        try:\n",
    "#            model_cv = CatBoostRegressor(**param)\n",
    "#            model_cv.fit(X_t, y_t, eval_set=[(X_v, y_v)])\n",
    "#            preds = model_cv.predict(X_v)\n",
    "#            fold_rmse = root_mean_squared_error(y_v, preds)\n",
    "#            rmses.append(fold_rmse)\n",
    "#        except Exception as e:\n",
    "#             print(f\"Warning: Optuna trial fold {fold+1} failed: {e}\")\n",
    "#             return np.inf \n",
    "#    if not rmses: return np.inf\n",
    "#    return np.mean(rmses)\n",
    "#\n",
    "#study_focused = optuna.create_study(direction='minimize')\n",
    "#print(f\"Running Focused Optuna study for {N_TRIALS_OPTUNA_FOCUSED} trials...\")\n",
    "#study_focused.optimize(objective_focused, n_trials=N_TRIALS_OPTUNA_FOCUSED)\n",
    "#\n",
    "#print(\"Focused Optuna Optimization Finished.\")\n",
    "#best_params_optuna_focused = {}\n",
    "#best_value_optuna_focused = np.inf\n",
    "#final_model_optuna_focused = None\n",
    "#optuna_focused_model_metrics = {'RMSE': np.inf, 'MAPE': np.inf, 'R2': -np.inf}\n",
    "#\n",
    "#if study_focused.best_trial:\n",
    "#    trial_focused = study_focused.best_trial\n",
    "#    best_value_optuna_focused = trial_focused.value\n",
    "#    print(f\"  Value (RMSE): {best_value_optuna_focused:.4f}\")\n",
    "#    print(\"  Best Parameters Found by Focused Optuna: \")\n",
    "#    best_params_optuna_focused = trial_focused.params\n",
    "#\n",
    "#    print(json.dumps(best_params_optuna_focused, indent=4))\n",
    "#    with open('best_params_opt_gold_mon.json', \"w\") as f:\n",
    "#        json.dump(best_params_optuna_focused,f, indent=4)\n",
    "#else:\n",
    "#     print(\"Focused Optuna study did not complete successfully or found no best trial.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION SANS PRUNING\n",
    "#with open('best_params_opt_gold_mon.json', \"r\") as f:\n",
    "#        best_params_optuna_focused= json.load(f)\n",
    "#quant_idx = best_params_optuna_focused.pop(\"quant_idx\")\n",
    "#mono_idx  = best_params_optuna_focused.pop(\"mono_idx\")\n",
    "#\n",
    "## 2) map back into your pre-built lists\n",
    "#best_params_optuna_focused[\"per_float_feature_quantization\"] = quantisation_candidates[quant_idx]\n",
    "#best_params_optuna_focused[\"monotone_constraints\"]            = monotone_constraint_candidates[mono_idx]\n",
    "#print(best_params_optuna_focused)\n",
    "#final_model_optuna_focused = CatBoostRegressor(\n",
    "#    cat_features=cat_variables, loss_function='RMSE', eval_metric='RMSE', random_seed=RANDOM_STATE, **best_params_optuna_focused)\n",
    "#final_model_optuna_focused.fit(X_rfecv, y_train, eval_set=(X_rfecv_test, y_test), early_stopping_rounds=200, verbose=0, plot=True)\n",
    "#final_model_optuna_focused.save_model(\n",
    "#    \"models/focused_optuna_model_no_target.cbm\"    # filename\n",
    "#)\n",
    "#print(\"\\n--- Focused Optuna Final Model Evaluation ---\")\n",
    "#y_pred_optuna_focused = final_model_optuna_focused.predict(X_rfecv_test)\n",
    "#rmse_optuna_focused = root_mean_squared_error(y_test, y_pred_optuna_focused)\n",
    "#mape_optuna_focused = mean_absolute_percentage_error(y_test, y_pred_optuna_focused) * 100\n",
    "#r2_optuna_focused = r2_score(y_test, y_pred_optuna_focused)\n",
    "#print(f\"Test RMSE: {rmse_optuna_focused:.2f}, MAPE: {mape_optuna_focused:.2f}%, R2: {r2_optuna_focused:.4f}\")\n",
    "#optuna_focused_model_metrics = {'RMSE': rmse_optuna_focused, 'MAPE': mape_optuna_focused, 'R2': r2_optuna_focused}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89918fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION MULTIPLES NOTEBOOKS\n",
    "#print(\"\\n\" + \"=\"*50)\n",
    "#print(\"--- Overall Model Comparison & Analysis (with CV Evaluation) ---\")\n",
    "#\n",
    "## Imports for CV and metrics\n",
    "#from sklearn.model_selection import KFold, cross_validate\n",
    "#from sklearn.metrics import make_scorer, root_mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "#from catboost import CatBoostRegressor, Pool, cv as catboost_cv # Ensure Pool is imported\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import os # Ensure os is imported\n",
    "#\n",
    "#\n",
    "#\n",
    "## 2) Define the full list of categorical features (including date)\n",
    "#cat_features = cats_rfecv\n",
    "#\n",
    "## 3) Build a single Pool over your training data with cat_features\n",
    "#train_pool = Pool(data=X_rfecv, label=y_train, cat_features=cat_features)\n",
    "#\n",
    "## --- Define CV strategy ---------------------------------------------\n",
    "#N_CV_EVAL_FOLDS = 10\n",
    "#print(f\"Using {N_CV_EVAL_FOLDS}-Fold Cross-Validation for model evaluation metrics.\")\n",
    "#\n",
    "## --- Load models and run CatBoost’s native cv() ---------------------\n",
    "#models_to_load_paths = {\n",
    "#    \"Baseline\": 'models/cb_base_model.cbm',\n",
    "#    \"TimeAware\": 'models/cb_time_model.cbm',\n",
    "#    \"Golden\":    'models/cb_golden_model.cbm',\n",
    "#    \"RS_tuned\":  'models/RS_model_mon_gold.cbm',\n",
    "#    \"optuna\":    'models/focused_optuna_model_no_target.cbm',\n",
    "#    \"optuna_prun\": 'models/focused_optuna_prun.cbm'\n",
    "#}\n",
    "#\n",
    "#model_cv_metrics = {}\n",
    "#loaded_models = {}\n",
    "#for name, path in models_to_load_paths.items():\n",
    "#    try:\n",
    "#        # load the model hyper-params\n",
    "#        m = CatBoostRegressor()\n",
    "#        m.load_model(path)\n",
    "#        loaded_models[name] = model\n",
    "#        print(f\"Loaded '{name}', running CatBoost.cv …\")\n",
    "#\n",
    "#        # grab the params and inject eval_metric + custom_metric\n",
    "#        params = m.get_params()\n",
    "#        # primary metric = RMSE (matches your loss_function), extras = MAPE and R2\n",
    "#        params['eval_metric']   = 'RMSE'\n",
    "#        params['custom_metric'] = ['MAPE','R2']\n",
    "#\n",
    "#        # run the 10-fold CV\n",
    "#        cv_df = catboost_cv(\n",
    "#            pool=train_pool,\n",
    "#            params=params,\n",
    "#            fold_count=N_CV_EVAL_FOLDS,\n",
    "#            partition_random_seed=RANDOM_STATE,\n",
    "#            shuffle=True,\n",
    "#            as_pandas=True,\n",
    "#            verbose=False\n",
    "#        )\n",
    "#        # cv_df columns include:\n",
    "#        #   'iterations',\n",
    "#        #   'test-RMSE-mean', 'test-RMSE-std',\n",
    "#        #   'test-MAPE-mean','test-MAPE-std',\n",
    "#        #   'test-R2-mean','test-R2-std', etc.\n",
    "#\n",
    "#        # 2) Pull out the final means from the last row:\n",
    "#        mean_rmse = cv_df['test-RMSE-mean'].iloc[-1]\n",
    "#        mean_mape = cv_df['test-MAPE-mean'].iloc[-1] * 100     # percent\n",
    "#        mean_r2   = cv_df['test-R2-mean'].iloc[-1]\n",
    "#\n",
    "#        model_cv_metrics[name] = {\n",
    "#            'RMSE': mean_rmse,\n",
    "#            'MAPE': mean_mape,\n",
    "#            'R2':   mean_r2\n",
    "#        }\n",
    "#        print(f\"  → {name}: RMSE={mean_rmse:.4f}, MAPE={mean_mape:.2f}%, R²={mean_r2:.4f}\")\n",
    "#\n",
    "#    except Exception as e:\n",
    "#        print(f\"Error evaluating '{name}': {e}. Skipping.\")\n",
    "#\n",
    "#\n",
    "#print(\"\\nCombining all models for comparison...\")\n",
    "#all_models_to_compare = loaded_models.copy() \n",
    "#\n",
    "## --- Create Metrics DataFrame (using MEAN CV metrics) ---\n",
    "#print(\"\\nCreating final metrics comparison table using Mean CV Scores...\")\n",
    "#if not model_cv_metrics:\n",
    "#     print(\"Warning: No model CV metrics available for comparison table.\")\n",
    "#     comparison_df_all = pd.DataFrame() # Empty DataFrame\n",
    "#else:\n",
    "#    # build DataFrame and keep only RMSE/ MAP E/ R2\n",
    "#    comparison_df_all = pd.DataFrame(model_cv_metrics).T\n",
    "#    cols_to_show = [c for c in ['RMSE','MAPE','R2'] if c in comparison_df_all.columns]\n",
    "#    comparison_df_all = comparison_df_all[cols_to_show]\n",
    "#    print(\"\\nCombined Model Performance Comparison (Mean CV Scores on Training Data):\")\n",
    "#    print(comparison_df_all.sort_values('RMSE').to_string(float_format=\"%.4f\"))\n",
    "#    \n",
    "#    # save JSON\n",
    "#    METRICS_FILENAME = 'final_metrics.json'\n",
    "#    print(f\"Saving final CV metrics comparison to {METRICS_FILENAME}...\")\n",
    "#    comparison_df_all.to_json(METRICS_FILENAME, indent=4, orient=\"index\")\n",
    "#\n",
    "## --- Run CatBoost `.compare()` on the TEST SET ---\n",
    "## Note: .compare() still uses the single test split for direct comparison\n",
    "#print(\"\\n--- Comparing CatBoost Models using model.compare() on Test Set ---\")\n",
    "#\n",
    "#    # Create comparison pool using the original TEST set\n",
    "#\n",
    "#X_rfecv_test['date'] = X_rfecv_test['date'].astype(str)\n",
    "#comparison_pool = Pool(data=X_rfecv_test, label=y_test, cat_features=cat_features)\n",
    "#model_names = list(all_models_to_compare.keys())\n",
    "#for i in range(len(model_names)):\n",
    "#    for j in range(i+1, len(model_names)):\n",
    "#        n1, n2 = model_names[i], model_names[j]\n",
    "#        m1 = all_models_to_compare[n1]\n",
    "#        m2 = all_models_to_compare[n2]\n",
    "#        print(f\"\\nComparing '{n1}' vs '{n2}' on Test Set…\")\n",
    "#        try:\n",
    "#            comp_df = m1.compare(\n",
    "#                m2,\n",
    "#                comparison_pool,\n",
    "#                metrics=['RMSE']     # you can also add 'MAPE','R2' here if desired\n",
    "#            )\n",
    "#            print(comp_df[['metric_name','model1_wins','model2_wins','metric_value_diff']])\n",
    "#        except Exception as e:\n",
    "#            print(f\"  Could not compare {n1} vs {n2}: {e}\")\n",
    "#print(\"=\"*50 + \"\\n\")\n",
    "#\n",
    "#\n",
    "## --- Final Model Analysis (Choose Overall Best based on CV Metrics Table) ---\n",
    "#print(\"\\n--- Final Model Analysis (Overall Best based on CV Metrics) ---\")\n",
    "#final_model_to_analyze = None\n",
    "#best_model_name = None\n",
    "#\n",
    "## Use the comparison_df_all which now contains CV metrics\n",
    "#if not comparison_df_all.empty:\n",
    "#     try:\n",
    "#        best_name = comparison_df_all['RMSE'].idxmin()\n",
    "#        print(f\"Selected '{best_name}' as best model (lowest Mean CV RMSE).\")\n",
    "#        final_model_to_analyze = all_models_to_compare[best_name]\n",
    "#     except Exception as e:\n",
    "#          print(f\"Could not determine best model from CV metrics table: {e}\")\n",
    "#\n",
    "## Analyze the selected best model (trained on full training data)\n",
    "#if final_model_to_analyze and final_model_to_analyze.is_fitted():\n",
    "#    print(f\"\\nAnalyzing model: {best_model_name}\")\n",
    "#    # Feature Importance (using the model trained on full X_rfecv)\n",
    "#    print(\"\\nPlotting native feature importance...\")\n",
    "#    if 'X_rfecv' in locals() and hasattr(X_rfecv, 'columns'):\n",
    "#        final_feature_names = X_rfecv.columns.tolist()\n",
    "#        # Assuming plot_native_feature_importance is available from utils\n",
    "#        # You might want to disable plotting in Azure ML jobs or save plots to output dir\n",
    "#        try:\n",
    "#             from utils import plot_native_feature_importance\n",
    "#             # plot_native_feature_importance(final_model_to_analyze, final_feature_names) # Comment out plt.show() in utils for Azure ML\n",
    "#             print(\"Native importance plot generation skipped for Azure ML script.\")\n",
    "#        except ImportError:\n",
    "#             print(\"Could not import plot_native_feature_importance from utils.\")\n",
    "#    else:\n",
    "#        print(\"Warning: Cannot get feature names for native importance plot.\")\n",
    "#\n",
    "#    # SHAP Analysis (Optional - can be computationally expensive)\n",
    "#    print(\"\\nGenerating SHAP plots...\")\n",
    "#    if 'X_rfecv_test' in locals() and 'final_feature_names' in locals():\n",
    "#        try:\n",
    "#            from utils import plot_shap_summary, plot_shap_waterfall\n",
    "#            plot_shap_summary(final_model_to_analyze, X_rfecv_test, final_feature_names, plot_type=\"bar\") # Comment out plt.show()\n",
    "#            plot_shap_summary(final_model_to_analyze, X_rfecv_test, final_feature_names, plot_type=\"dot\") # Comment out plt.show()\n",
    "#            plot_shap_waterfall(final_model_to_analyze, X_rfecv_test, final_feature_names, instance_index=1) # Comment out plt.show()\n",
    "#            print(\"SHAP plot generation skipped for Azure ML script.\")\n",
    "#        except ImportError:\n",
    "#             print(\"Could not import SHAP plotting functions from utils.\")\n",
    "#    else:\n",
    "#        print(\"Warning: Cannot generate SHAP plots (X_rfecv_test or feature names not available?).\")\n",
    "#else:\n",
    "#    print(\"No final best model available for analysis (either none found or selection failed).\")\n",
    "#\n",
    "#print(\"\\n\" + \"=\"*50)\n",
    "#print(\"Script section finished.\")\n",
    "## %%\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958549ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# pondérer différentes stations car sinon non-représentatif\n",
    "# soit prendre individuellement chacune variables climatiques, soit pondérer\n",
    "# à voir avec la feature selection\n",
    "\n",
    "from meteostat import Point, Daily\n",
    "import pandas as pd, os, joblib\n",
    "\n",
    "CACHE_FILE = \"data/_wx_cache.joblib\"\n",
    "\n",
    "def fetch_station_series(lat=48.8566, lon=2.3522): # station de Paris\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        daily = joblib.load(CACHE_FILE)\n",
    "        daily.index = daily.index.tz_localize(None)\n",
    "        daily.index.name = 'date'\n",
    "        return daily.reset_index()\n",
    "\n",
    "    start = pd.to_datetime(df['date']).min().to_pydatetime().replace(tzinfo=None)\n",
    "    end   = pd.to_datetime(df['date']).max().to_pydatetime().replace(tzinfo=None)\n",
    "\n",
    "    point = Point(lat, lon)\n",
    "    daily = Daily(point, start=start, end=end).fetch()\n",
    "\n",
    "    keep = [c for c in ['wspd','tmin','tmax'] if c in daily.columns]\n",
    "    daily = daily[keep].rename(columns={'wspd':'windspeed'})\n",
    "\n",
    "    if 'tmin' in daily.columns and 'tmax' in daily.columns:\n",
    "        daily['t_range'] = daily['tmax'] - daily['tmin']\n",
    "        daily = daily.drop(columns=['tmin','tmax'])\n",
    "\n",
    "    daily.index = daily.index.tz_localize(None)\n",
    "    daily.index.name = 'date'\n",
    "\n",
    "    joblib.dump(daily, CACHE_FILE)\n",
    "    return daily.reset_index()\n",
    "\n",
    "# re-fetch & merge\n",
    "wx = fetch_station_series()\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date']).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "wx['date'] = pd.to_datetime(wx['date']).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "df = (df.merge(wx, on='date', how='left').assign(hdd_wind=lambda x: x['hdd'] * (1 + x['windspeed']/10)))'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
